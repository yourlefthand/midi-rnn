{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lib.csv_to_array import convert as pre_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# George Frederick Handel (robot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[0, 0, Header, format, nTracks, division]\n",
    "[Track, 0, Start_track]\n",
    "[Track, Time, Note_off_c, Channel, Note, Velocity]\n",
    "```\n",
    "checkout http://www.fourmilab.ch/webtools/midicsv/ for more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i.e: \n",
      "['1', ' 190', ' Note_on_c', ' 0', ' 68', ' 0']\n"
     ]
    }
   ],
   "source": [
    "gigue = './csv/handel_hwv-433_5_gigue_(c)yamada.mid.csv'\n",
    "# gigue = './csv/total.csv'\n",
    "\n",
    "ga = pre_process(gigue)\n",
    "\n",
    "print \"i.e: \\n\" + str(ga[32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's drop the time bit for now\n",
    "whatevs = [g.pop(1) for g in ga]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are fewer than 3 unique track in piece with nearly 2522 events!\n",
      "there are fewer than 12 unique action in piece with nearly 2522 events!\n",
      "there are fewer than 44 unique channel in piece with nearly 2522 events!\n",
      "there are fewer than 48 unique pitch in piece with nearly 2522 events!\n",
      "there are fewer than 8 unique velocity in piece with nearly 2522 events!\n",
      "there are fewer than 3 unique hurh? in piece with nearly 2522 events!\n"
     ]
    }
   ],
   "source": [
    "#padding \n",
    "s_len = max([len(g) for g in ga])\n",
    "\n",
    "p_ga = [g + [None] * (s_len - len(g)) for g in ga]\n",
    "\n",
    "elems = ['track', 'action', 'channel', 'pitch', 'velocity', 'hurh?']\n",
    "\n",
    "v = []\n",
    "s = []\n",
    "iv = []\n",
    "l_v = []\n",
    "d_l = len(p_ga)\n",
    "\n",
    "for i in range(s_len):\n",
    "    v.append([g[i] for g in p_ga])\n",
    "    s.append(set(v[i]))\n",
    "    iv.append([(ind, na) for ind, na in enumerate(s[i])])\n",
    "    l_v.append(len(s[i]))\n",
    "    print(\"there are fewer than \" + str(l_v[i]) \n",
    "          + \" unique \" + elems[i] + \" in piece with nearly \"\n",
    "          + str(d_l) + \" events!\")\n",
    "\n",
    "ix_to_val = lambda y, f: [x for x in f if x[0] == y][0][1]\n",
    "val_to_ix = lambda y, f: [x for x in f if x[1] == y][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include crossover on the hidden levels - the last ones.\n",
    "And we're going to ignore the time element until we can implement a 'sequence generation' network.\n",
    "\n",
    "Open Questions:\n",
    " - does normalizing the length of each vocabulary make it easier to pure numpy arrays?\n",
    "    - I kinda think numpy arrays in lists viz a viz list comprehensions is not such a bad thing\n",
    " - what about sharing?\n",
    "    - my feeling is that each element will have len(elements) hidden weight matrices, and we will be applying them against the element's hidden layer -- do we need hidden layers, plural?\n",
    "      my instinct is: no - we will be defining the hidden layer as:\n",
    "      tanh(dot(in-wgts, in-vals) + dot(last-self-weights, last-self) + dot(last-other-weights, last-other) ... etc etc. This fits the analogy - we don't really need to prefer any previous values, but we DO want to be sensitive to temporal patterns in every dimension. Let's do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the fun begins\n",
    "import numpy as np\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# weights belong only to the individual network, values do not\n",
    "# model parameters\n",
    "weights = [{'Wxh':np.random.randn(hidden_size, l_v[i])*0.01,\n",
    "            'Whh':np.random.randn(s_len, hidden_size, hidden_size)*0.01,\n",
    "            'Why':np.random.randn(l_v[i], hidden_size)*0.01,\n",
    "            'bh':np.zeros((s_len, hidden_size, 1)),\n",
    "            'by':np.zeros((l_v[i], 1))} for i in range(s_len)] # input to hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's do the main loop next\n",
    "n = 0 #iteration counter\n",
    "p = 0 #position in `events` array, `nm` - more generally `p_ga`\n",
    "\n",
    "m_weights = [{'mWxh':np.zeros_like(w['Wxh']),\n",
    "              'mWhh':np.zeros_like(w['Whh']),\n",
    "              'mWhy':np.zeros_like(w['Why']),\n",
    "              'mbh':np.zeros_like(w['bh']),\n",
    "              'mby':np.zeros_like(w['by'])} for w in weights]\n",
    "\n",
    "#hm - does loss belong to each network individually?\n",
    "smooth_loss = [-1 * np.log(1.0/l_v[i]) * seq_length for i in range(s_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (6,100,100) into shape (100,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-191-f8e937796cbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m#one is hot! note the cool use of indexing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             hs[j][t] = np.tanh(np.dot(weights[j]['Wxh'], xs[j][t]) + sum(\n\u001b[1;32m---> 32\u001b[1;33m                     [np.dot(weights[j]['Whh'][i], hs[j][t-1]) for i in range(s_len)]) + weights[j]['bh'])\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0mys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Why'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'by'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#unnormed log prob for next word\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#sigmoid those probs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (6,100,100) into shape (100,1)"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    #position is absolute - invariant across elements\n",
    "    if p + seq_length + 1 >= d_l or n == 0: #beinning & end: training data\n",
    "        #hprev = [np.zeros((hidden_size,1)) for i in range(s_len)] #marks the start/end of seq\n",
    "        hprev = np.zeros((s_len, hidden_size,1))\n",
    "        p = 0 #start at the very beginning\n",
    "        \n",
    "    #transform first chunk of data into indices\n",
    "    inputs = [[val_to_ix(nn, iv[ind]) for nn in n[p:p+seq_length]] for ind,n in enumerate(v)]\n",
    "    #transform shifted+1 chunk of data for target vals\n",
    "    targets = [[val_to_ix(nn, iv[ind]) for nn in n[p+1:p+seq_length+1]] for ind,n in enumerate(v)]\n",
    "    \n",
    "    #run the current seq through the net, and fetch the gradients\n",
    "    #not so pretty - let's not use this in favor of an array of np arrays - \n",
    "    #must be so since input/output lengths are variant\n",
    "    #let's see what happens - and use plain old numpy with normalized \n",
    "    xs = [np.zeros((seq_length, l)) for l in l_v] # python voodoo for initializing these vals - would numpy arr work better?\n",
    "    hs = [np.zeros((seq_length, hidden_size, hidden_size,1)) for l in l_v]\n",
    "    ys = [np.zeros((seq_length, max_v)) for l in l_v] # output states\n",
    "    ps = [np.zeros((seq_length, max_v)) for l in l_v] # probabilities\n",
    "    \n",
    "    #I wonder what it would look like to have a hs with len less than seq_length\n",
    "    \n",
    "    loss = [] + [0] * s_len#does each element experience loss separately? probably\n",
    "    \n",
    "    #forward pass!\n",
    "    #ew this is gross. can we do it purely with numpy if they're different sizes?\n",
    "    for j in xrange(s_len):\n",
    "        for t in xrange(len(inputs)): #1 for each val in seq - remind u of rk4?\n",
    "            xs[j][t,inputs[t]] = 1 #one is hot! note the cool use of indexing\n",
    "            hs[j][t] = np.tanh(np.dot(weights[j]['Wxh'], xs[j][t]) + sum(\n",
    "                    np.dot(weights[j]['Whh'][i], hs[j][t-1]) for i in range(s_len)) + weights[j]['bh'])\n",
    "            ys[j][t] = np.dot(weights[j]['Why'], hs[j][t]) + weights[j]['by'] #unnormed log prob for next word\n",
    "            ps[j][t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) #sigmoid those probs\n",
    "            #apply loss - log linear\n",
    "            loss[j] += -np.log(ps[j][t,targets[j][t]])\n",
    "        \n",
    "    #backward pass! - might want to split this when we've got parallelized\n",
    "    #interconnected networks\n",
    "    \n",
    "    d_weights = [{'dWxh':np.zeros_like(w['Wxh']),\n",
    "              'dWhh':np.zeros_like(w['Whh']),\n",
    "              'dWhy':np.zeros_like(w['Why']),\n",
    "              'dbh':np.zeros_like(w['bh']),\n",
    "              'dby':np.zeros_like(w['by'])} for w in weights]\n",
    "    \n",
    "    dhnext = [np.zeros_like(h[0]) for h in hs]\n",
    "    dhraw = []\n",
    "\n",
    "    dy = [np.copy(p) for p in ps] # python voodoo for initializing these vals - would numpy arr work better?\n",
    "    dh = [np.zeros_like(h) for h in hs]\n",
    "    dx = [np.zeros_like(x) for x in xs] # output states\n",
    "    \n",
    "    #that's right, we backprop to every sequence we visit - t has power\n",
    "    for j in xrange(s_len):\n",
    "        for t in reversed(xrange(len(inputs))):\n",
    "            dy[j][targets[t]] -= 1 #actual value 'applied' to space, will fix\n",
    "            d_weights[j]['dWhy'] += np.dot(dy[j], hs[j][t].T) #bp into y weights\n",
    "            d_weights[j]['dby'] += dy[j]\n",
    "            dh[j] = np.dot(weights[j]['Why'].T, dy[j]) + dhnext[j] #bp into h\n",
    "            dhraw.append((1-hs[j][t]*hs[j][t]) * dh[j]) #take bp and filter via tanh(u)`\n",
    "            d_weights[j]['dbh'] += dhraw[j] #makes sense, the bias accums via the bp of output\n",
    "            d_weights[j]['dWxh'] += np.dot(dhraw[j], xs[j][t].T) #bp hidden into inputs weights\n",
    "            d_weights[j]['dWhh'] += np.dot(dhraw[j], hs[j][t-1].T) #bp into t-1 hidden state weights\n",
    "            dhnext[j] = np.dot(d_weights[j]['Whh'].T, dhraw[j])\n",
    "        for keys in d_weights[j]:\n",
    "            np.clip(d_weights[j][key],-5,5,out=d_weights[j][key]) #clip to mitigate exploding grad\n",
    "\n",
    "    smooth_loss = [ss * 0.999 + loss[i] * 0.001 for i,ss in smooth_loss] #wow, such voodoo\n",
    "    \n",
    "    if n % 100 == 0:\n",
    "        print('iter %d, loss %f,' % (n, sl) for sl in smooth_loss) #see that loss shrinkin?\n",
    "    \n",
    "    #adagrad - let's make this thing it's own bit?!\n",
    "    #seems like afterthought, Karpathy!\n",
    "    #such a business, but gives us tuple:\n",
    "    #(cur-weights, del-weights, mem-weights? wat r mem-weights?)\n",
    "    for j in range(s_len):\n",
    "        for param, dparam, mem in zip(['Wxh', 'Whh', 'Why', 'bh', 'by'],\n",
    "                                      ['dWxh', 'dWhh', 'dWhy', 'dbh', 'dby'],\n",
    "                                      ['mWxh', 'mWhh', 'mWhy', 'mbh', 'mby']):\n",
    "            m_weights[j][mem] += d_weights[j][dparam] * d_weights[j][dparam] #square delta weights into memory? y square?\n",
    "            weights[j][param] += -learning_rate * d_weights[j][dparam] / np.sqrt(m_weights[j][mem] + 1e-8) #adagrad!\n",
    "        \n",
    "    p += seq_length\n",
    "    n += 1 \n",
    "    #let's go again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generator - basically a forward only run\n",
    "#   \"\"\" \n",
    "#   sample a sequence of integers from the model \n",
    "#   h is memory state, seed_ix is seed letter for first time step\n",
    "#   \"\"\"\n",
    "def sample(h, seed_ix, n):\n",
    "    x = np.zeros((vocab_size,1))\n",
    "    x[seed_ix] = 1 #builds initial input - catalyst\n",
    "    ixes = [] #sequence out!\n",
    "    #forward pass, tidy loop\n",
    "    for t in xrange(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        #p= option sets the probabilities of 'random' choice\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_ix = sample(hprev, inputs[0], 3500)\n",
    "sample_notes = [note_for_ind(i)[1] for i in sample_ix]\n",
    "print sample_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

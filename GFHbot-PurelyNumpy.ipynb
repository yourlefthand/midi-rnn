{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lib.csv_to_array import convert as pre_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# George Frederick Handel (robot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[0, 0, Header, format, nTracks, division]\n",
    "[Track, 0, Start_track]\n",
    "[Track, Time, Note_off_c, Channel, Note, Velocity]\n",
    "```\n",
    "checkout http://www.fourmilab.ch/webtools/midicsv/ for more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i.e: \n",
      "['1', ' 190', ' Note_on_c', ' 0', ' 68', ' 0']\n"
     ]
    }
   ],
   "source": [
    "gigue = './csv/handel_hwv-433_5_gigue_(c)yamada.mid.csv'\n",
    "# gigue = './csv/total.csv'\n",
    "\n",
    "ga = pre_process(gigue)\n",
    "\n",
    "print \"i.e: \\n\" + str(ga[32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's drop the time bit for now\n",
    "whatevs = [g.pop(1) for g in ga]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#padding \n",
    "s_len = max([len(g) for g in ga])\n",
    "\n",
    "p_ga = [g + [None] * (s_len - len(g)) for g in ga]\n",
    "\n",
    "# p_ga = [[g[3]] for g in p_ga]\n",
    "# s_len = 1\n",
    "\n",
    "elems = ['track', 'action', 'channel', 'pitch', 'velocity', 'hurh?']\n",
    "\n",
    "v = []\n",
    "s = []\n",
    "d_l = len(p_ga)\n",
    "\n",
    "for i in range(s_len):\n",
    "    v.append([g[i] for g in p_ga])\n",
    "    s.append(set(v[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's pad the sets of values with None until they reach max(l_v)\n",
    "#this will give us a matrix of values length_data X max(length_vocab)\n",
    "m_v = max(len(ss) for ss in s)\n",
    "\n",
    "p_s = [list(ss) + [None] * (m_v - len(ss)) for ss in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ix_to_val = lambda y, f: f[y]\n",
    "val_to_ix = lambda y, f: f.index(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#pure numerics - we don't need to worry about how they map back until later\n",
    "train_data = np.asarray([[val_to_ix(vvv, p_s[i]) for vvv in vv] \n",
    "                  for i, vv in enumerate(v)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include crossover on the hidden levels - the last ones.\n",
    "And we're going to ignore the time element until we can implement a 'sequence generation' network.\n",
    "\n",
    "Open Questions:\n",
    " - does normalizing the length of each vocabulary make it easier to pure numpy arrays?\n",
    "    - I kinda think numpy arrays in lists viz a viz list comprehensions is not such a bad thing\n",
    " - what about sharing?\n",
    "    - my feeling is that each element will have len(elements) hidden weight matrices, and we will be applying them against the element's hidden layer -- do we need hidden layers, plural?\n",
    "      my instinct is: no - we will be defining the hidden layer as:\n",
    "      tanh(dot(in-wgts, in-vals) + dot(last-self-weights, last-self) + dot(last-other-weights, last-other) ... etc etc. This fits the analogy - we don't really need to prefer any previous values, but we DO want to be sensitive to temporal patterns in every dimension. Let's do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the fun begins - let's see if we can avoid using anything but numpy arrays!\n",
    "#note: the real challenge is that the length of the input vectors & weight matrices are not always the same!\n",
    "#do we pad them with null values? probably - but what null values to use without breaking the format?\n",
    "#None is taken for most - but we'll use it as the padding value in teh vocabs since our lambda's only grab the first\n",
    "#result. We'll also try to mask the Wxh and Wyh weights to zero for these connections - right?\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# weights belong only to the individual network, values do not\n",
    "# model parameters\n",
    "Wxh = np.random.randn(s_len, hidden_size, m_v)*0.01\n",
    "Whh = np.random.randn(s_len, s_len, hidden_size, hidden_size)*0.01\n",
    "Why = np.random.randn(s_len, m_v, hidden_size,1)*0.01\n",
    "bh = np.zeros((s_len, hidden_size,1))\n",
    "by = np.zeros((s_len, m_v,1)) # input to hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's do the main loop next\n",
    "n = 0 #iteration counter\n",
    "p = 0 #position in `events` array, `nm` - more generally `p_ga`\n",
    "\n",
    "mWxh = np.zeros_like(Wxh)\n",
    "mWhh = np.zeros_like(Whh)\n",
    "mWhy = np.zeros_like(Why)\n",
    "mbh = np.zeros_like(bh)\n",
    "mby = np.zeros_like(by)\n",
    "\n",
    "#hm - does loss belong to each network individually?\n",
    "smooth_loss = np.zeros((s_len))\n",
    "smooth_loss[:] = -1 * np.log(1.0/m_v) * seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    #position is absolute - invariant across elements\n",
    "    if p + seq_length + 1 >= d_l or n == 0: #beinning & end: training data\n",
    "        #hprev = [np.zeros((hidden_size,1)) for i in range(s_len)] #marks the start/end of seq\n",
    "        hprev = np.zeros((s_len, hidden_size,1))\n",
    "        p = 0 #start at the very beginning\n",
    "        \n",
    "    #transform first chunk of data into indices\n",
    "    inputs = train_data[:,p:p+seq_length]\n",
    "    #transform shifted+1 chunk of data for target vals\n",
    "    targets = train_data[:,p+1:p+seq_length+1]\n",
    "    \n",
    "    #run the current seq through the net, and fetch the gradients\n",
    "    #not so pretty - let's not use this in favor of an array of np arrays - \n",
    "    #must be so since input/output lengths are variant\n",
    "    #let's see what happens - and use plain old numpy with normalized \n",
    "    xs = np.zeros((s_len, seq_length, m_v,1))    \n",
    "    hs = np.zeros((s_len, seq_length, hidden_size,1))\n",
    "    ys = np.zeros((s_len, seq_length, m_v,1)) # output states\n",
    "    ps = np.zeros((s_len, seq_length, m_v,1)) # probabilities\n",
    "    \n",
    "    #I wonder what it would look like to have a hs with len less than seq_length\n",
    "    hs[:,-1] = hprev[:]\n",
    "    loss = np.zeros((s_len))#does each element experience loss separately? probably\n",
    "    \n",
    "    #forward pass!\n",
    "    #sequences are tough! what how does the sequence need to be trandformed\n",
    "    for t in np.arange(inputs.shape[1]): #remind u of rk4?\n",
    "        for j in np.arange(inputs.shape[0]):\n",
    "            xs[j,t,inputs[j,t]] = 1 \n",
    "            hs[j,t] = np.tanh(np.dot(Wxh[j], xs[j,t]) + \n",
    "                              np.sum(np.dot(Whh[j,i], hs[i,t-1]) for i in np.arange(inputs.shape[0]))+\n",
    "                              bh[j])\n",
    "            ys[j,t] = np.dot(Why[j,...,0], hs[j,t]) +by[j] #unnormed log prob for next word\n",
    "            ps[j,t] = np.exp(ys[j,t]) / np.sum(np.exp(ys[j,t])) #sigmoid those probs\n",
    "            #apply loss - log linear\n",
    "            loss[j] += -np.log(ps[j,t,targets[j,t]])\n",
    "        \n",
    "    #backward pass! - might want to split this when we've got parallelized\n",
    "    #interconnected networks\n",
    "    \n",
    "    dWxh = np.zeros_like(Wxh)\n",
    "    dWhh = np.zeros_like(Whh)\n",
    "    dWhy = np.zeros_like(Why)\n",
    "    dbh = np.zeros_like(bh)\n",
    "    dby = np.zeros_like(by)\n",
    "    \n",
    "    dhnext = np.zeros_like(hs[:,0])\n",
    "    \n",
    "    #jus cuz it's neater, less in the loop\n",
    "    dy = np.copy(ps)\n",
    "    dh = np.zeros_like(hs)\n",
    "    \n",
    "    #that's right, we backprop to every sequence we visit - t has power\n",
    "    for t in reversed(np.arange(inputs.shape[1])):\n",
    "        for j in np.arange(inputs.shape[0]):\n",
    "            dy[j, t,targets[j,t]] -= 1 #actual value 'applied' to space, will fix\n",
    "            dWhy[j,...,0] += np.dot(dy[j,t], hs[j,t].T) #bp into y weights\n",
    "            dby[j] += dy[j,t]\n",
    "            dh[j,t] = np.dot(Why[j].T, dy[j,t]) + dhnext[j] #bp into h\n",
    "            dhraw = (1-hs[j,t]*hs[j,t]) * dh[j,t] #take bp and filter via tanh(u)`\n",
    "            dbh[j] += dhraw #makes sense, the bias accums via the bp of output\n",
    "            dWxh[j] += np.dot(dhraw, xs[j][t].T) #bp hidden into inputs weights\n",
    "            dWhh[j] += np.asarray([np.dot(dhraw, hs[i,t-1].T) for i in np.arange(inputs.shape[0])]) #bp into t-1 hidden state weights\n",
    "            dhnext[j] = np.dot(Whh[j,j], dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam,-5,5,out=dparam) #clip to mitigate exploding grad\n",
    "\n",
    "    smooth_loss = [ss * 0.999 + loss[i] * 0.001 for i,ss in enumerate(smooth_loss)] #wow, such voodoo\n",
    "    \n",
    "    if n % 10000 == 0:\n",
    "        print('iter %d: ' % (n) + str(['loss %f,' % (sl) for sl in smooth_loss])) #see that loss shrinkin?\n",
    "    \n",
    "    #adagrad - let's make this thing its own bit?!\n",
    "    #seems like afterthought, Karpathy!\n",
    "    #such a business, but gives us tuple:\n",
    "    #(cur-weights, del-weights, mem-weights? wat r mem-weights?)\n",
    "    for j in np.arange(inputs.shape[0]):\n",
    "        for param, dparam, mem in zip([Wxh[j], Whh[j], Why[j], bh[j], by[j]],\n",
    "                                      [dWxh[j], dWhh[j], dWhy[j], dbh[j], dby[j]],\n",
    "                                      [mWxh[j], mWhh[j], mWhy[j], mbh[j], mby[j]]):\n",
    "            mem += dparam * dparam #square delta weights into memory? y square?\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) #adagrad!\n",
    "        \n",
    "    p += seq_length\n",
    "    n += 1 \n",
    "    #let's go again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generator - basically a forward only run\n",
    "#   \"\"\" \n",
    "#   sample a sequence of integers from the model \n",
    "#   h is memory state, seed_ix is seed letter for first time step\n",
    "#   \"\"\"\n",
    "def sample(h, seed_ix, n):\n",
    "    x = np.zeros((vocab_size,1))\n",
    "    x[seed_ix] = 1 #builds initial input - catalyst\n",
    "    ixes = [] #sequence out!\n",
    "    #forward pass, tidy loop\n",
    "    for t in xrange(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        #p= option sets the probabilities of 'random' choice\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_ix = sample(hprev, inputs[0], 3500)\n",
    "sample_notes = [note_for_ind(i)[1] for i in sample_ix]\n",
    "print sample_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

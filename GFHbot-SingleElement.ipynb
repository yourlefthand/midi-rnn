{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lib.csv_to_array import convert as pre_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# George Frederick Handel (robot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[0, 0, Header, format, nTracks, division]\n",
    "[Track, 0, Start_track]\n",
    "[Track, Time, Note_off_c, Channel, Note, Velocity]\n",
    "```\n",
    "checkout http://www.fourmilab.ch/webtools/midicsv/ for more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i.e: \n",
      "['1', ' 190', ' Note_on_c', ' 0', ' 68', ' 0']\n"
     ]
    }
   ],
   "source": [
    "gigue = './csv/handel_hwv-433_5_gigue_(c)yamada.mid.csv'\n",
    "# gigue = './csv/total.csv'\n",
    "\n",
    "ga = pre_process(gigue)\n",
    "\n",
    "print \"i.e: \\n\" + str(ga[32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll be defining a recurrant neural network that takes, as input, a midi 'sentence' - such as the one above - and will guess at the output from a 'word-level' - i.e. it will output a similar row of length 6.\n",
    "\n",
    "we will be training on every element of this array, but they have different behaviors.\n",
    "(for each index, starting at index 0 as 1):\n",
    "   1. Track - because these are keyboard suites, this will be constant in this notebook (only one instrument!) but we will treat this as a 'word-set' with ```length(set(midi[:,0]))```\n",
    "   1. Time - is not a word-set, it is a monotonically increasing value and will require somewhat special attention because we always need a numeric value > last numeric value by some arbitrary (learned) step?\n",
    "      1. alternatively - make a word set of midi[:,1] - midi[:-1,1] - that should give us a pretty rich set, and shoul be sufficient. Or atleast make a set range(max(midi[:,1]))\n",
    "   1. Action - yay for baroque music, the only real action here is 'Note_on_c' (cuz we are tapping that clavichord) but there are headers too. I'm interested to see how the recurrence will handle this\n",
    "   1. Channel - another word-set, should be simple\n",
    "   1. Pitch - another word-set, I'm curious about how key changes between pieces (i.e. word-sets with some limited degree of overlap) will be handled as we expand this to train on more music\n",
    "   1. Velocity - let's treat this as a word-set...see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about what each statement we input.\n",
    "Simply put, we'll build a set of the keys present in each 'statement' - for most, not all of the values presented. This is for two reasons:\n",
    "   - many of the values are limited by their medium (*Note* for example)\n",
    "      - interesting caveat: do we train this to 'learn a piece' or 'learn an instrument'\n",
    "        if the former, the vocabulary can be inferred from the set of data available. Cool!\n",
    "        this also means that we would need to learn more pieces to come up with cool new\n",
    "        ideas. After learning only one piece, we could only compose using the keys available\n",
    "        to that piece. On the one hand, this is disappointing, because you'd think we'd want\n",
    "        to experience the 'key' of the piece as an unspoken affector - such that we attempt to\n",
    "        play all notes in the 'key' (or even all notes whatsoever) and learn which to play - \n",
    "        i.e. learning the instrument.\n",
    "        On the other, we would focus on the essence of the piece, and would build a repertoire\n",
    "        as we understand how to place pieces. Additionally, a feedback run of the system - or \n",
    "        perhaps two identical systems passing errors to one another - could be used to \n",
    "        'compose' - still we run the risk of overfitting.\n",
    "        Finally, when learning new 'pieces' we do not initialize the vocabulary as we did with\n",
    "        the first piece, instead we expand the set, preserve existing weights and initialize\n",
    "        new ones. this will be some very interesting code - but we must MUST beware of\n",
    "        overfitting! My intuition is that any overfitting we experience will dissolve as more \n",
    "        pieces are learned, but it would be a shame if we couldn't get the machine to\n",
    "        understand the distinction between styles, jumping from one to another on a common\n",
    "        note or remaining trapped in a bad key. However, the length of the sequence compounded \n",
    "        on the hidden layers will hopefully prevent this - or make it delightfully improbable\n",
    "   - some values are sequences (i.e. the time step, especially if we consider timestep @\n",
    "     n = timestep @ n - 1 plus some value @ n <= length of the piece. Perhaps we could\n",
    "     could consider these 'vocabularies' of real numbers - but this stinks too much of\n",
    "     of overfitting and would result in such mechanical performances. let's instead allow\n",
    "     our system to choose a real number, and we'll test its error not against probability, but \n",
    "     against the real error of similarity between the prescribed step and our guess. This\n",
    "     could be pretty neat, and maintains our analogy of 'nascent musician'-ship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's think about what the construct of each statement we will output - in fact, it's a series of separate rnns - one for each 'term' of the statement, each with its own unique vocabulary, each with one (or more!) hidden layers of arbitrary depth and full (or parametrically managed) connectivity with - their initial layers, subsequent hidden layers (if any), some weighted sequence of previous hidden layers (namely, input values and output weights) and their output layers. Does the sequence of previous layers need more than one layer? can we learn which previous sequences to prefer? It would be nice if this could provide some sense of flourish, of intention.\n",
    "Finally, the output will be in the form of a vector length of vocabulary of each index's likelihood, checked for error against the next value in the sequence - the subsequent input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's dick around with just one predictor for now, including lstm or gated memories for that \n",
    "layer. To that end, it would benefit us to make this extensible enough to make it easy to\n",
    "connect the hidden layers or other inputs (wait, can't the hidden layers be the same size?!)\n",
    "and the pre-sequential hidden layers of those inputs. This will give us a lovely level of\n",
    "connectivity as well as the 'sanctity' of data that had concerned us with the 'mixed bag' of \n",
    "data approach. maybe it would work anyway? mgiht be worth investigating\n",
    "\n",
    "how are we going to handle the 'headers' shall we be setting them ourselves? let's read them \n",
    "anyway, I believe in learning...nevermind, we an't read them, they have alternate rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are fewer than 48 unique notes in piece with nearly 2522 events!\n"
     ]
    }
   ],
   "source": [
    "# let's pad\n",
    "\n",
    "#padding \n",
    "s_len = max([len(g) for g in ga])\n",
    "\n",
    "inval_action = ['']\n",
    "\n",
    "p_ga = [g + [None] * (s_len - len(g)) for g in ga]\n",
    "\n",
    "notes = [g[4] for g in p_ga]\n",
    "\n",
    "n = set(notes)\n",
    "nm = [(ind, na) for ind, na in enumerate(n)]\n",
    "\n",
    "vocab_size, data_length = len(n), len(ga)\n",
    "\n",
    "print(\"there are fewer than \" + str(vocab_size) \n",
    "      + \" unique notes in piece with nearly \"\n",
    "      + str(data_length) + \" events!\")\n",
    "\n",
    "note_for_ind = lambda y: [x for x in nm if x[0] == y][0]\n",
    "ind_for_note = lambda y: [x for x in nm if x[1] == y][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the fun begins\n",
    "import numpy as np\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## borrowed courtesy http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "## by way of https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "## \"\"\"\n",
    "##   inputs,targets are both list of integers.\n",
    "##   hprev is Hx1 array of initial hidden state\n",
    "##   returns the loss, gradients on model parameters, and last hidden state\n",
    "##   \"\"\"\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    xs = {} # one-hots of inputs\n",
    "    hs = {} # hidden states\n",
    "    ys = {} # output states\n",
    "    ps = {} # probabilities\n",
    "    \n",
    "    hs[-1] = np.copy(hprev) #how many hidden states u want to go back?\n",
    "    loss = 0 #hello beautiful - ah, the perfection if the unstarted\n",
    "    \n",
    "    #forward pass!\n",
    "    for t in xrange(len(inputs)): #1 for each val in seq - remind u of rk4?\n",
    "        xs[t] = np.zeros((vocab_size,1)) # 1-of-k shaped\n",
    "        xs[t][inputs[t]] = 1 #one is hot! note the cool use of indexing\n",
    "        #ooh, sexy - we're getting the hidden state by weighing the dot \n",
    "        #prods of the input -> hidden and last_hidden -> hidden\n",
    "        # THAT'S WHY WE SET THE LAST HIDDEN TO IND -1!!!! MEMORY!\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh)\n",
    "        ys[t] = np.dot(Why, hs[t]) + by #unnormed log prob for next word\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) #sigmoid those probs\n",
    "        #apply loss - log linear\n",
    "        loss += -np.log(ps[t][targets[t],0])\n",
    "        \n",
    "    #backward pass! - might want to split this when we've got parallelized\n",
    "    #interconnected networks\n",
    "    \n",
    "    dWxh = np.zeros_like(Wxh) #scope alert!\n",
    "    dWhh = np.zeros_like(Whh) #scope alert! - need to package these for\n",
    "    dWhy = np.zeros_like(Why) #scope alert! - parallelization\n",
    "    \n",
    "    dbh = np.zeros_like(bh) #scope alert!\n",
    "    dby = np.zeros_like(by) #scope alert!\n",
    "    \n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    \n",
    "    #that's right, we backprop to every sequence we visit - t has power\n",
    "    for t in reversed(xrange(len(inputs))):\n",
    "        dy = np.copy(ps[t]) #y grab smoothed prob? - its where we leave nn\n",
    "        dy[targets[t]] -= 1 #actual value 'applied' to space, will fix\n",
    "        dWhy += np.dot(dy, hs[t].T) #bp into y weights\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext #bp into h\n",
    "        dhraw = (1-hs[t]*hs[t]) * dh #take bp and filter via tanh(u)`\n",
    "        dbh += dhraw #makes sense, the bias accums via the bp of output\n",
    "        dWxh += np.dot(dhraw, xs[t].T) #bp hidden into inputs weights\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T) #bp into t-1 hidden state weights\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5,5, out=dparam) #clip to mitigate exploding grad\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generator - basically a forward only run\n",
    "#   \"\"\" \n",
    "#   sample a sequence of integers from the model \n",
    "#   h is memory state, seed_ix is seed letter for first time step\n",
    "#   \"\"\"\n",
    "def sample(h, seed_ix, n):\n",
    "    x = np.zeros((vocab_size,1))\n",
    "    x[seed_ix] = 1 #builds initial input - catalyst\n",
    "    ixes = [] #sequence out!\n",
    "    #forward pass, tidy loop\n",
    "    for t in xrange(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        #p= option sets the probabilities of 'random' choice\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lots of definitions here\n",
    "n = 0 #iteration counter\n",
    "p = 0 #position in `events` array, `nm` - more generally `p_ga`\n",
    "\n",
    "mWxh = np.zeros_like(Wxh) #needed by adagrad, could be generalized?\n",
    "mWhh = np.zeros_like(Whh) #ditto\n",
    "mWhy = np.zeros_like(Why) #ditto\n",
    "mbh = np.zeros_like(bh) #ditto\n",
    "mby = np.zeros_like(by) #ditto\n",
    "\n",
    "smooth_loss = -1 * np.log(1.0/vocab_size) * seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss 96.780038,\n",
      "iter 100, loss 100.105122,\n",
      "iter 200, loss 99.519638,\n",
      "iter 300, loss 98.606640,\n",
      "iter 400, loss 97.555256,\n",
      "iter 500, loss 96.454854,\n",
      "iter 600, loss 95.302661,\n",
      "iter 700, loss 94.060039,\n",
      "iter 800, loss 92.711923,\n",
      "iter 900, loss 91.307091,\n",
      "iter 1000, loss 89.762375,\n",
      "iter 1100, loss 88.105645,\n",
      "iter 1200, loss 86.335657,\n",
      "iter 1300, loss 84.516296,\n",
      "iter 1400, loss 82.657501,\n",
      "iter 1500, loss 80.770168,\n",
      "iter 1600, loss 78.866536,\n",
      "iter 1700, loss 76.949395,\n",
      "iter 1800, loss 75.170179,\n",
      "iter 1900, loss 73.374703,\n",
      "iter 2000, loss 71.671364,\n",
      "iter 2100, loss 69.815565,\n",
      "iter 2200, loss 67.955738,\n",
      "iter 2300, loss 66.141716,\n",
      "iter 2400, loss 64.390396,\n",
      "iter 2500, loss 62.723512,\n",
      "iter 2600, loss 61.040793,\n",
      "iter 2700, loss 59.311900,\n",
      "iter 2800, loss 57.570052,\n",
      "iter 2900, loss 55.923399,\n",
      "iter 3000, loss 54.354960,\n",
      "iter 3100, loss 52.765734,\n",
      "iter 3200, loss 51.231508,\n",
      "iter 3300, loss 49.794897,\n",
      "iter 3400, loss 48.331195,\n",
      "iter 3500, loss 46.939484,\n",
      "iter 3600, loss 45.595768,\n",
      "iter 3700, loss 44.303080,\n",
      "iter 3800, loss 42.964582,\n",
      "iter 3900, loss 41.746094,\n",
      "iter 4000, loss 40.479360,\n",
      "iter 4100, loss 39.227755,\n",
      "iter 4200, loss 38.163306,\n",
      "iter 4300, loss 37.107128,\n",
      "iter 4400, loss 36.054912,\n",
      "iter 4500, loss 35.115680,\n",
      "iter 4600, loss 34.092017,\n",
      "iter 4700, loss 33.096733,\n",
      "iter 4800, loss 32.116413,\n",
      "iter 4900, loss 31.173049,\n",
      "iter 5000, loss 30.254885,\n",
      "iter 5100, loss 29.363881,\n",
      "iter 5200, loss 28.493748,\n",
      "iter 5300, loss 27.673298,\n",
      "iter 5400, loss 26.878410,\n",
      "iter 5500, loss 26.071458,\n",
      "iter 5600, loss 25.298010,\n",
      "iter 5700, loss 24.599323,\n",
      "iter 5800, loss 24.038915,\n",
      "iter 5900, loss 23.448423,\n",
      "iter 6000, loss 22.822345,\n",
      "iter 6100, loss 22.176049,\n",
      "iter 6200, loss 21.533862,\n",
      "iter 6300, loss 20.914781,\n",
      "iter 6400, loss 20.395122,\n",
      "iter 6500, loss 20.073809,\n",
      "iter 6600, loss 19.606556,\n",
      "iter 6700, loss 19.118121,\n",
      "iter 6800, loss 18.588734,\n",
      "iter 6900, loss 18.127055,\n",
      "iter 7000, loss 17.666321,\n",
      "iter 7100, loss 17.365993,\n",
      "iter 7200, loss 17.023847,\n",
      "iter 7300, loss 16.644725,\n",
      "iter 7400, loss 16.190312,\n",
      "iter 7500, loss 15.707669,\n",
      "iter 7600, loss 15.349982,\n",
      "iter 7700, loss 14.945885,\n",
      "iter 7800, loss 14.717304,\n",
      "iter 7900, loss 14.296913,\n",
      "iter 8000, loss 13.899444,\n",
      "iter 8100, loss 13.528938,\n",
      "iter 8200, loss 13.120111,\n",
      "iter 8300, loss 12.748085,\n",
      "iter 8400, loss 12.347291,\n",
      "iter 8500, loss 12.152680,\n",
      "iter 8600, loss 11.934149,\n",
      "iter 8700, loss 11.589757,\n",
      "iter 8800, loss 11.246317,\n",
      "iter 8900, loss 10.954256,\n",
      "iter 9000, loss 10.876279,\n",
      "iter 9100, loss 10.586130,\n",
      "iter 9200, loss 10.284723,\n",
      "iter 9300, loss 10.046641,\n",
      "iter 9400, loss 9.763016,\n",
      "iter 9500, loss 9.697536,\n",
      "iter 9600, loss 9.777288,\n",
      "iter 9700, loss 9.595653,\n",
      "iter 9800, loss 9.346638,\n",
      "iter 9900, loss 9.092826,\n",
      "iter 10000, loss 8.827307,\n",
      "iter 10100, loss 8.723605,\n",
      "iter 10200, loss 8.505324,\n",
      "iter 10300, loss 8.223198,\n",
      "iter 10400, loss 7.955555,\n",
      "iter 10500, loss 7.743406,\n",
      "iter 10600, loss 7.567902,\n",
      "iter 10700, loss 7.383247,\n",
      "iter 10800, loss 7.381973,\n",
      "iter 10900, loss 7.254141,\n",
      "iter 11000, loss 7.129782,\n",
      "iter 11100, loss 6.990853,\n",
      "iter 11200, loss 6.829748,\n",
      "iter 11300, loss 6.652967,\n",
      "iter 11400, loss 6.472167,\n",
      "iter 11500, loss 6.279687,\n",
      "iter 11600, loss 6.071515,\n",
      "iter 11700, loss 5.884896,\n",
      "iter 11800, loss 5.754967,\n",
      "iter 11900, loss 5.687972,\n",
      "iter 12000, loss 5.623500,\n",
      "iter 12100, loss 5.477472,\n",
      "iter 12200, loss 5.311029,\n",
      "iter 12300, loss 5.153952,\n",
      "iter 12400, loss 4.993050,\n",
      "iter 12500, loss 4.843472,\n",
      "iter 12600, loss 4.709543,\n",
      "iter 12700, loss 4.595518,\n",
      "iter 12800, loss 4.560982,\n",
      "iter 12900, loss 5.318466,\n",
      "iter 13000, loss 5.823974,\n",
      "iter 13100, loss 6.107799,\n",
      "iter 13200, loss 6.109515,\n",
      "iter 13300, loss 5.938527,\n",
      "iter 13400, loss 5.713844,\n",
      "iter 13500, loss 5.487991,\n",
      "iter 13600, loss 5.272164,\n",
      "iter 13700, loss 5.101396,\n",
      "iter 13800, loss 4.930957,\n",
      "iter 13900, loss 4.764608,\n",
      "iter 14000, loss 4.617685,\n",
      "iter 14100, loss 4.460358,\n",
      "iter 14200, loss 4.299691,\n",
      "iter 14300, loss 4.148076,\n",
      "iter 14400, loss 4.016385,\n",
      "iter 14500, loss 3.889208,\n",
      "iter 14600, loss 3.767927,\n",
      "iter 14700, loss 3.652059,\n",
      "iter 14800, loss 3.539185,\n",
      "iter 14900, loss 3.450774,\n",
      "iter 15000, loss 3.501551,\n",
      "iter 15100, loss 3.517402,\n",
      "iter 15200, loss 3.666265,\n",
      "iter 15300, loss 3.658341,\n",
      "iter 15400, loss 3.643205,\n",
      "iter 15500, loss 3.559070,\n",
      "iter 15600, loss 3.500294,\n",
      "iter 15700, loss 3.435008,\n",
      "iter 15800, loss 3.355622,\n",
      "iter 15900, loss 3.260809,\n",
      "iter 16000, loss 3.161025,\n",
      "iter 16100, loss 3.065320,\n",
      "iter 16200, loss 2.971379,\n",
      "iter 16300, loss 2.883465,\n",
      "iter 16400, loss 2.800927,\n",
      "iter 16500, loss 2.722468,\n",
      "iter 16600, loss 2.650620,\n",
      "iter 16700, loss 2.582149,\n",
      "iter 16800, loss 2.519080,\n",
      "iter 16900, loss 2.461097,\n",
      "iter 17000, loss 2.420015,\n",
      "iter 17100, loss 2.382963,\n",
      "iter 17200, loss 2.350355,\n",
      "iter 17300, loss 2.322639,\n",
      "iter 17400, loss 2.657367,\n",
      "iter 17500, loss 2.890428,\n",
      "iter 17600, loss 3.194603,\n",
      "iter 17700, loss 3.395591,\n",
      "iter 17800, loss 3.387214,\n",
      "iter 17900, loss 3.316178,\n",
      "iter 18000, loss 3.198814,\n",
      "iter 18100, loss 3.086610,\n",
      "iter 18200, loss 2.970058,\n",
      "iter 18300, loss 2.858158,\n",
      "iter 18400, loss 2.751513,\n",
      "iter 18500, loss 2.651766,\n",
      "iter 18600, loss 2.558467,\n",
      "iter 18700, loss 2.471730,\n",
      "iter 18800, loss 2.390930,\n",
      "iter 18900, loss 2.319194,\n",
      "iter 19000, loss 2.275150,\n",
      "iter 19100, loss 2.223651,\n",
      "iter 19200, loss 2.178657,\n",
      "iter 19300, loss 2.148255,\n",
      "iter 19400, loss 2.104543,\n",
      "iter 19500, loss 2.052145,\n",
      "iter 19600, loss 2.001119,\n",
      "iter 19700, loss 1.951759,\n",
      "iter 19800, loss 1.905074,\n",
      "iter 19900, loss 1.861629,\n",
      "iter 20000, loss 1.830898,\n",
      "iter 20100, loss 1.812672,\n",
      "iter 20200, loss 1.787165,\n",
      "iter 20300, loss 1.763795,\n",
      "iter 20400, loss 1.743548,\n",
      "iter 20500, loss 1.715221,\n",
      "iter 20600, loss 2.548919,\n",
      "iter 20700, loss 3.314737,\n",
      "iter 20800, loss 3.586577,\n",
      "iter 20900, loss 3.663650,\n",
      "iter 21000, loss 3.598685,\n",
      "iter 21100, loss 3.627114,\n",
      "iter 21200, loss 3.517824,\n",
      "iter 21300, loss 3.359822,\n",
      "iter 21400, loss 3.194988,\n",
      "iter 21500, loss 3.039004,\n",
      "iter 21600, loss 2.891845,\n",
      "iter 21700, loss 2.754117,\n",
      "iter 21800, loss 2.626414,\n",
      "iter 21900, loss 2.507815,\n",
      "iter 22000, loss 2.398148,\n",
      "iter 22100, loss 2.296735,\n",
      "iter 22200, loss 2.202955,\n",
      "iter 22300, loss 2.116297,\n",
      "iter 22400, loss 2.036163,\n",
      "iter 22500, loss 1.962211,\n",
      "iter 22600, loss 1.894202,\n",
      "iter 22700, loss 1.832306,\n",
      "iter 22800, loss 1.780924,\n",
      "iter 22900, loss 2.300835,\n",
      "iter 23000, loss 2.514089,\n",
      "iter 23100, loss 2.612758,\n",
      "iter 23200, loss 2.558152,\n",
      "iter 23300, loss 2.468623,\n",
      "iter 23400, loss 2.371600,\n",
      "iter 23500, loss 2.271162,\n",
      "iter 23600, loss 2.176956,\n",
      "iter 23700, loss 2.088112,\n",
      "iter 23800, loss 2.004789,\n",
      "iter 23900, loss 1.926078,\n",
      "iter 24000, loss 1.852650,\n",
      "iter 24100, loss 1.784691,\n",
      "iter 24200, loss 1.721796,\n",
      "iter 24300, loss 1.663730,\n",
      "iter 24400, loss 1.610284,\n",
      "iter 24500, loss 1.561174,\n",
      "iter 24600, loss 1.520003,\n",
      "iter 24700, loss 1.487301,\n",
      "iter 24800, loss 1.473270,\n",
      "iter 24900, loss 1.490692,\n",
      "iter 25000, loss 1.596955,\n",
      "iter 25100, loss 1.613639,\n",
      "iter 25200, loss 1.590036,\n",
      "iter 25300, loss 1.569710,\n",
      "iter 25400, loss 1.534009,\n",
      "iter 25500, loss 1.502103,\n",
      "iter 25600, loss 1.464103,\n",
      "iter 25700, loss 1.425224,\n",
      "iter 25800, loss 1.390837,\n",
      "iter 25900, loss 1.364496,\n",
      "iter 26000, loss 1.335570,\n",
      "iter 26100, loss 1.308387,\n",
      "iter 26200, loss 1.284311,\n",
      "iter 26300, loss 1.259799,\n",
      "iter 26400, loss 1.236727,\n",
      "iter 26500, loss 1.215210,\n",
      "iter 26600, loss 1.196467,\n",
      "iter 26700, loss 1.183239,\n",
      "iter 26800, loss 1.180784,\n",
      "iter 26900, loss 1.179299,\n",
      "iter 27000, loss 1.252646,\n",
      "iter 27100, loss 1.282399,\n",
      "iter 27200, loss 1.284857,\n",
      "iter 27300, loss 1.282197,\n",
      "iter 27400, loss 1.286804,\n",
      "iter 27500, loss 1.279627,\n",
      "iter 27600, loss 1.255514,\n",
      "iter 27700, loss 1.231388,\n",
      "iter 27800, loss 1.208518,\n",
      "iter 27900, loss 1.185999,\n",
      "iter 28000, loss 1.162460,\n",
      "iter 28100, loss 1.142681,\n",
      "iter 28200, loss 1.122569,\n",
      "iter 28300, loss 1.106352,\n",
      "iter 28400, loss 1.087157,\n",
      "iter 28500, loss 1.070830,\n",
      "iter 28600, loss 1.054140,\n",
      "iter 28700, loss 1.041558,\n",
      "iter 28800, loss 1.026536,\n",
      "iter 28900, loss 1.014354,\n",
      "iter 29000, loss 1.001438,\n",
      "iter 29100, loss 0.991825,\n",
      "iter 29200, loss 0.980493,\n",
      "iter 29300, loss 0.971803,\n",
      "iter 29400, loss 0.962588,\n",
      "iter 29500, loss 0.953917,\n",
      "iter 29600, loss 0.947923,\n",
      "iter 29700, loss 0.940755,\n",
      "iter 29800, loss 0.933027,\n",
      "iter 29900, loss 0.937629,\n",
      "iter 30000, loss 1.102709,\n",
      "iter 30100, loss 1.731111,\n",
      "iter 30200, loss 2.179402,\n",
      "iter 30300, loss 2.813229,\n",
      "iter 30400, loss 2.973075,\n",
      "iter 30500, loss 2.919090,\n",
      "iter 30600, loss 2.853495,\n",
      "iter 30700, loss 2.739662,\n",
      "iter 30800, loss 2.621550,\n",
      "iter 30900, loss 2.494499,\n",
      "iter 31000, loss 2.364317,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6124e93725a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m#run the current seq through the net, and fetch the gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0msmooth_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.999\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.001\u001b[0m \u001b[1;31m#wow, such voodoo\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-468e6ce20092>\u001b[0m in \u001b[0;36mlossFun\u001b[1;34m(inputs, targets, hprev)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mdWxh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWxh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#scope alert!\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mdWhh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#scope alert! - need to package these for\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mdWhy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#scope alert! - parallelization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kal/.virtualenvs/jupyter/local/lib/python2.7/site-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36mzeros_like\u001b[1;34m(a, dtype, order, subok)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubok\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;31m# needed instead of a 0 to get same result as zeros for for string dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m     \u001b[0mmultiarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unsafe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#loop this shit - forevah?\n",
    "while True:\n",
    "    #sweeping events in steps seq_length long\n",
    "    if p + seq_length + 1 >= data_length or n == 0: #beinning & end: training data\n",
    "        hprev = np.zeros((hidden_size,1)) #empty memory - save instead?\n",
    "        p = 0 #start at the very beginning\n",
    "        \n",
    "    #transform first chunk of data into indices\n",
    "    inputs = [ind_for_note(no)[0] for no in notes[p:p+seq_length]]\n",
    "    #transform shifted+1 chunk of data for target vals\n",
    "    targets = [ind_for_note(no)[0] for no in notes[p+1:p+seq_length+1]]\n",
    "    \n",
    "    #grab a sample every now and again\n",
    "    if n % 100 == 0:\n",
    "        #using the 'current' hidden layer, grab the current letter and \n",
    "        #give me 200 more\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        sample_notes = [note_for_ind(i)[1] for i in sample_ix]\n",
    "        #print sample_notes #what good does this do me?\n",
    "    \n",
    "    #run the current seq through the net, and fetch the gradients\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001 #wow, such voodoo\n",
    "    \n",
    "    if n % 100 == 0:\n",
    "        print 'iter %d, loss %f,' % (n, smooth_loss) #see that loss shrinkin?\n",
    "    \n",
    "    #adagrad - let's make this thing it's own bit?!\n",
    "    #seems like afterthought, Karpathy!\n",
    "    #such a business, but gives us tuple:\n",
    "    #(cur-weights, del-weights, mem-weights? wat r mem-weights?)\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam #square delta weights into memory? y square?\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) #adagrad!\n",
    "        \n",
    "    p += seq_length\n",
    "    n += 1 \n",
    "    #let's go again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_ix = sample(hprev, inputs[0], 3500)\n",
    "sample_notes = [note_for_ind(i)[1] for i in sample_ix]\n",
    "print sample_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
